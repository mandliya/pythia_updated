{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install ninja yacs cython matplotlib demjson\n",
    "!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd vqa-maskrcnn-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo python setup.py build\n",
    "!sudo python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/rmandli_g_clemson_edu/pythia')\n",
    "sys.path.append('/home/rmandli_g_clemson_edu/pythia/vqa-maskrcnn-benchmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo conda install pandas -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from ipywidgets import widgets, Layout\n",
    "from io import BytesIO\n",
    "\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from maskrcnn_benchmark.layers import nms\n",
    "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n",
    "\n",
    "from pythia.utils.configuration import ConfigNode\n",
    "from pythia.tasks.processors import VocabProcessor, VQAAnswerProcessor\n",
    "from pythia.tasks.processors import FastTextProcessor, SoftCopyAnswerProcessor, SimpleWordProcessor\n",
    "from pythia.models import LoRRA\n",
    "from pythia.common.registry import registry\n",
    "from pythia.common.sample import Sample, SampleList\n",
    "from pythia.tasks.vqa.vqa2 import VQA2Dataset\n",
    "from pythia.tasks.concat_dataset import PythiaConcatDataset\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rmandli_g_clemson_edu/pythia\n",
      "configs/vqa/textvqa/lorra.yml\r\n"
     ]
    }
   ],
   "source": [
    "%cd /home/rmandli_g_clemson_edu/pythia\n",
    "\n",
    "!ls configs/vqa/textvqa/lorra.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_path = '/home/rmandli_g_clemson_edu/pythia'\n",
    "lorra_model_config_path = '/home/rmandli_g_clemson_edu/pythia/configs/vqa/textvqa/lorra.yml'\n",
    "detectron_model_config_path = '/home/rmandli_g_clemson_edu/pythia/configs/detectron_model/detectron_model.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-02 20:04:15--  https://dl.fbaipublicfiles.com/pythia/data/vocabulary_100k.txt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 626738 (612K) [text/plain]\n",
      "Saving to: ‘data/vocabulary_100k.txt’\n",
      "\n",
      "data/vocabulary_100 100%[===================>] 612.05K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2019-06-02 20:04:16 (17.4 MB/s) - ‘data/vocabulary_100k.txt’ saved [626738/626738]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O data/vocabulary_100k.txt https://dl.fbaipublicfiles.com/pythia/data/vocabulary_100k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-02 20:11:08--  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.yaml\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:16a6, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 918 [text/plain]\n",
      "Saving to: ‘configs/detectron_model/detectron_model.yaml’\n",
      "\n",
      "configs/detectron_m 100%[===================>]     918  --.-KB/s    in 0s      \n",
      "\n",
      "2019-06-02 20:11:08 (18.1 MB/s) - ‘configs/detectron_model/detectron_model.yaml’ saved [918/918]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O configs/detectron_model/detectron_model.yaml https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-02 20:14:05--  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.pth\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:16a6, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 684079216 (652M) [application/octet-stream]\n",
      "Saving to: ‘data/detectron/model/detectron_model.pth’\n",
      "\n",
      "data/detectron/mode 100%[===================>] 652.39M  90.4MB/s    in 7.8s    \n",
      "\n",
      "2019-06-02 20:14:14 (83.4 MB/s) - ‘data/detectron/model/detectron_model.pth’ saved [684079216/684079216]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O data/detectron/model/detectron_model.pth  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.pth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVQADemo:\n",
    "    TARGET_IMAGE_SIZE = [448, 448]\n",
    "    CHANNEL_MEAN = [0.485, 0.456, 0.406]\n",
    "    CHANNEL_STD = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._init_processors()\n",
    "        self.lorra_model = self._build_lorra_model()\n",
    "        self.detection_model = self._build_detection_model()\n",
    "        self.resnet_model = self._build_resnet_model()\n",
    "        \n",
    "    def _init_processors(self):\n",
    "        \"\"\"\n",
    "        Pythia uses processors is to keep data processing pipelines as similar as\n",
    "        possible for different datasets and allow code reusability.\n",
    "        \"\"\"\n",
    "        with open(\"configs/vqa/textvqa/lorra.yml\") as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "            \n",
    "\n",
    "        #update config with includes for model specific config\n",
    "        for inc in config.get(\"includes\", []):\n",
    "            config.update(yaml.load(open(\"pythia/\"+ inc), Loader=yaml.FullLoader))\n",
    "\n",
    "        config = ConfigNode(config)\n",
    "        config.datasets = 'textvqa'\n",
    "        config.training_parameters.evalai_inference = True\n",
    "        textvqa_config = config.task_attributes.vqa.dataset_attributes.textvqa\n",
    "        \n",
    "        answer_processor_config = textvqa_config.processors.answer_processor\n",
    "        answer_processor_config.params.vocab_file = \\\n",
    "            \"data/vocabs/answers_textvqa_more_than_1.txt\"\n",
    "        self.answer_processor = SoftCopyAnswerProcessor(answer_processor_config.params)\n",
    "        print(\"self.answer_processor.get_vocab_size()\", self.answer_processor.get_vocab_size())\n",
    "    \n",
    "        registry.register(\"textvqa_num_final_outputs\", \n",
    "                      self.answer_processor.get_vocab_size())\n",
    "        registry.register(\"textvqa_answer_processor\", self.answer_processor)\n",
    "        \n",
    "        text_processor_config = textvqa_config.processors.text_processor\n",
    "        text_processor_config.params.vocab.vocab_file = \"data/vocabs/vocabulary_100k.txt\"\n",
    "        self.text_processor = VocabProcessor(text_processor_config.params)\n",
    "        registry.register(\"textvqa_text_processor\", self.text_processor)\n",
    "        registry.register(\"textvqa_text_vocab_size\", \n",
    "                      self.text_processor.get_vocab_size())\n",
    "        \n",
    "        ocr_token_processor_config = textvqa_config.processors.ocr_token_processor\n",
    "        self.ocr_token_processor = SimpleWordProcessor(ocr_token_processor_config)\n",
    "        registry.register(\"textvqa_ocr_token_processor\", self.ocr_token_processor)\n",
    "        #pprint(text_processor_config)\n",
    "        \n",
    "        context_processor_config = textvqa_config.processors.context_processor\n",
    "        self.context_processor = FastTextProcessor(context_processor_config.params)\n",
    "        registry.register(\"textvqa_context_processor\", self.context_processor)\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        registry.register(\"config\", config)\n",
    "        #pprint(config.classifier.keys())\n",
    "        \n",
    "    def _prepare_data_set(self, dataset_type, config):\n",
    "        if dataset_type not in config.imdb_files:\n",
    "            raise ValueError(\n",
    "                \"Dataset type {} is not present in \"\n",
    "                \"imdb_files of dataset config\".format(dataset_type)\n",
    "            )\n",
    "\n",
    "        imdb_files = self.config[\"imdb_files\"][dataset_type]\n",
    "\n",
    "        datasets = []\n",
    "\n",
    "        for imdb_idx in range(len(imdb_files)):\n",
    "            dataset = VQA2Dataset(dataset_type, imdb_idx, config)\n",
    "            datasets.append(dataset)\n",
    "\n",
    "        dataset = PythiaConcatDataset(datasets)\n",
    "\n",
    "        return dataset \n",
    "\n",
    "    def _add_imdb_details(self):\n",
    "        imdb_files = self.config.imdb_files\n",
    "        dataset_type = 'textvqa'\n",
    "        if dataset_type not in imdb_files:\n",
    "            raise ValueError(\n",
    "                \"Dataset type {} is not present in \"\n",
    "                \"imdb_files of dataset config\".format(dataset_type)\n",
    "            )\n",
    "\n",
    "        self.imdb_file = imdb_files[dataset_type][imdb_file_index]\n",
    "        self.imdb_file = self._get_absolute_path(self.imdb_file)\n",
    "        self.imdb = ImageDatabase(self.imdb_file)\n",
    "        \n",
    "    def _multi_gpu_state_to_single(self, state_dict):\n",
    "        new_sd = {}\n",
    "        for k, v in state_dict.items():\n",
    "            if not k.startswith('module.'):\n",
    "                raise TypeError(\"Not a multiple GPU state of dict\")\n",
    "            k1 = k[7:]\n",
    "            new_sd[k1] = v\n",
    "        return new_sd\n",
    "    \n",
    "\n",
    "    def _build_lorra_model(self):\n",
    "        state_dict = torch.load('data/models/lorra_best.pth')\n",
    "        #pprint(state_dict)\n",
    "        model_config = self.config.model_attributes.lorra\n",
    "        pprint(self.config.datasets)\n",
    "        model = LoRRA(model_config)\n",
    "        model.build()\n",
    "        model.init_losses_and_metrics()\n",
    "        self.model = model\n",
    "        #print(model.params)\n",
    "        if list(state_dict.keys())[0].startswith('module') and \\\n",
    "           not hasattr(model, 'module'):\n",
    "            state_dict = self._multi_gpu_state_to_single(state_dict)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(\"cuda\")\n",
    "        model.eval()\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def _build_resnet_model(self):\n",
    "        self.data_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.TARGET_IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.CHANNEL_MEAN, self.CHANNEL_STD),\n",
    "        ])\n",
    "        resnet152 = models.resnet152(pretrained=True)\n",
    "        resnet152.eval()\n",
    "        modules = list(resnet152.children())[:-2]\n",
    "        self.resnet152_model = torch.nn.Sequential(*modules)\n",
    "        self.resnet152_model.to(\"cuda\")\n",
    "        \n",
    "    def _build_detection_model(self):\n",
    "        cfg.merge_from_file('configs/detectron_model/detectron_model.yaml')\n",
    "        cfg.freeze()\n",
    "        model = build_detection_model(cfg)\n",
    "        checkpoint = torch.load('data/detectron/model/detectron_model.pth', \n",
    "                                  map_location=torch.device(\"cpu\"))\n",
    "        load_state_dict(model, checkpoint.pop(\"model\"))\n",
    "\n",
    "        model.to(\"cuda\")\n",
    "        model.eval()\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def add_ocr_details(self, sample_info, sample):\n",
    "        if self.use_ocr:\n",
    "            # Preprocess OCR tokens\n",
    "            ocr_tokens = [\n",
    "                self.ocr_token_processor({\"text\": token})[\"text\"]\n",
    "                for token in sample_info[\"ocr_tokens\"]\n",
    "            ]\n",
    "            # Get embeddings for tokens\n",
    "            context = self.context_processor({\"tokens\": ocr_tokens})\n",
    "            sample.context = context[\"text\"]\n",
    "            sample.context_tokens = context[\"tokens\"]\n",
    "            sample.context_feature_0 = context[\"text\"]\n",
    "            sample.context_info_0 = Sample()\n",
    "            sample.context_info_0.max_features = context[\"length\"]\n",
    "\n",
    "            order_vectors = torch.eye(len(sample.context_tokens))\n",
    "            order_vectors[context[\"length\"] :] = 0\n",
    "            sample.order_vectors = order_vectors\n",
    "\n",
    "        if self.use_ocr_info and \"ocr_info\" in sample_info:\n",
    "            sample.ocr_bbox = self.bbox_processor({\"info\": sample_info[\"ocr_info\"]})[\n",
    "                \"bbox\"\n",
    "            ]\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def predict(self, url, question):\n",
    "        with torch.no_grad():\n",
    "            detectron_features = self.get_detectron_features(url)\n",
    "            resnet_features = self.get_resnet_features(url)\n",
    "        \n",
    "            sample = Sample()\n",
    "\n",
    "            processed_text = self.text_processor({\"text\": question})\n",
    "            sample.text = processed_text[\"text\"]\n",
    "            sample.text_len = len(processed_text[\"tokens\"])\n",
    "\n",
    "            sample.image_feature_0 = detectron_features\n",
    "            sample.image_info_0 = Sample({\n",
    "              \"max_features\": torch.tensor(100, dtype=torch.long)\n",
    "            })\n",
    "            \n",
    "            \n",
    "            self._prepare_data_set(\"textvqa\", self.config.task_attributes.vqa.dataset_attributes.textvqa)\n",
    "            \n",
    "            # Get embeddings for tokens\n",
    "            context = self.context_processor({\"tokens\": processed_text[\"tokens\"]})\n",
    "            sample.context = context[\"text\"]\n",
    "            sample.context_tokens = context[\"tokens\"]\n",
    "            sample.context_feature_0 = context[\"text\"]\n",
    "            sample.context_info_0 = Sample()\n",
    "            sample.context_info_0.max_features = context[\"length\"]\n",
    "\n",
    "            order_vectors = torch.eye(len(sample.context_tokens))\n",
    "            order_vectors[context[\"length\"] :] = 0\n",
    "            sample.order_vectors = order_vectors\n",
    "\n",
    "            sample.image_feature_1 = resnet_features\n",
    "\n",
    "            sample_list = SampleList([sample])\n",
    "            sample_list = sample_list.to(\"cuda\")\n",
    "\n",
    "            scores = self.lorra_model(sample_list)[\"scores\"]\n",
    "            scores = torch.nn.functional.softmax(scores, dim=1)\n",
    "            actual, indices = scores.topk(5, dim=1)\n",
    "\n",
    "            top_indices = indices[0]\n",
    "            top_scores = actual[0]\n",
    "\n",
    "            probs = []\n",
    "            answers = []\n",
    "\n",
    "            for idx, score in enumerate(top_scores):\n",
    "                probs.append(score.item())\n",
    "                answers.append(\n",
    "                    self.answer_processor.idx2word(top_indices[idx].item())\n",
    "                )\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return probs, answers\n",
    "    \n",
    "    def get_actual_image(self, image_path):\n",
    "        if image_path.startswith('http'):\n",
    "            path = requests.get(image_path, stream=True).raw\n",
    "        else:\n",
    "            path = image_path\n",
    "        return path\n",
    "\n",
    "    def _image_transform(self, image_path):\n",
    "        path = self.get_actual_image(image_path)\n",
    "\n",
    "        img = Image.open(path)\n",
    "        im = np.array(img).astype(np.float32)\n",
    "        im = im[:, :, ::-1]\n",
    "        im -= np.array([102.9801, 115.9465, 122.7717])\n",
    "        im_shape = im.shape\n",
    "        im_size_min = np.min(im_shape[0:2])\n",
    "        im_size_max = np.max(im_shape[0:2])\n",
    "        im_scale = float(800) / float(im_size_min)\n",
    "        # Prevent the biggest axis from being more than max_size\n",
    "        if np.round(im_scale * im_size_max) > 1333:\n",
    "            im_scale = float(1333) / float(im_size_max)\n",
    "        im = cv2.resize(\n",
    "           im,\n",
    "           None,\n",
    "           None,\n",
    "           fx=im_scale,\n",
    "           fy=im_scale,\n",
    "           interpolation=cv2.INTER_LINEAR\n",
    "           )\n",
    "        img = torch.from_numpy(im).permute(2, 0, 1)\n",
    "        return img, im_scale\n",
    "    \n",
    "    def _process_feature_extraction(self, output,\n",
    "                                    im_scales,\n",
    "                                    feat_name='fc6',\n",
    "                                    conf_thresh=0.2):\n",
    "        batch_size = len(output[0][\"proposals\"])\n",
    "        n_boxes_per_image = [len(_) for _ in output[0][\"proposals\"]]\n",
    "        score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
    "        score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
    "        feats = output[0][feat_name].split(n_boxes_per_image)\n",
    "        cur_device = score_list[0].device\n",
    "\n",
    "        feat_list = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
    "            scores = score_list[i]\n",
    "\n",
    "            max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
    "\n",
    "            for cls_ind in range(1, scores.shape[1]):\n",
    "                cls_scores = scores[:, cls_ind]\n",
    "                keep = nms(dets, cls_scores, 0.5)\n",
    "                max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n",
    "                                            cls_scores[keep],\n",
    "                                            max_conf[keep])\n",
    "\n",
    "            keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n",
    "            feat_list.append(feats[i][keep_boxes])\n",
    "        return feat_list\n",
    "\n",
    "    def masked_unk_softmax(self, x, dim, mask_idx):\n",
    "        x1 = F.softmax(x, dim=dim)\n",
    "        x1[:, mask_idx] = 0\n",
    "        x1_sum = torch.sum(x1, dim=1, keepdim=True)\n",
    "        y = x1 / x1_sum\n",
    "        return y\n",
    "\n",
    "    def get_resnet_features(self, image_path):\n",
    "        path = self.get_actual_image(image_path)\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_transform = self.data_transforms(img)\n",
    "        \n",
    "        if img_transform.shape[0] == 1:\n",
    "            img_transform = img_transform.expand(3, -1, -1)\n",
    "        img_transform = img_transform.unsqueeze(0).to(\"cuda\")\n",
    "        \n",
    "        features = self.resnet152_model(img_transform).permute(0, 2, 3, 1)\n",
    "        features = features.view(196, 2048)\n",
    "        return features\n",
    "\n",
    "    def get_detectron_features(self, image_path):\n",
    "        im, im_scale = self._image_transform(image_path)\n",
    "        img_tensor, im_scales = [im], [im_scale]\n",
    "        current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "        current_img_list = current_img_list.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            output = self.detection_model(current_img_list)\n",
    "        feat_list = self._process_feature_extraction(output, im_scales, \n",
    "                                                    'fc6', 0.2)\n",
    "        return feat_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.answer_processor.get_vocab_size() 4047\n",
      "'textvqa'\n",
      "text_embeddings:  2048\n",
      "text_embeddings:  2048\n",
      "text_embeddings:  2048\n",
      "Multi Modal combine layer: 5000\n",
      "Classifier dimensions: 10000  out_dim 4047\n"
     ]
    }
   ],
   "source": [
    "demo = TextVQADemo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_widgets(url, question):\n",
    "    image_text = widgets.Text(\n",
    "        description=\"Image URL\", layout=Layout(minwidth=\"70%\")\n",
    "    )\n",
    "    question_text = widgets.Text(\n",
    "        description=\"Question\", layout=Layout(minwidth=\"70%\")\n",
    "    )\n",
    "\n",
    "    image_text.value = url\n",
    "    question_text.value = question\n",
    "    submit_button = widgets.Button(description=\"Ask TextVQA!\")\n",
    "\n",
    "    display(image_text)\n",
    "    display(question_text)\n",
    "    display(submit_button)\n",
    "\n",
    "    submit_button.on_click(lambda b: on_button_click(\n",
    "      b, image_text, question_text\n",
    "    ))\n",
    "\n",
    "    return image_text, question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_button_click(b, image_text, question_text):\n",
    "    clear_output()\n",
    "    image_path = demo.get_actual_image(image_text.value)\n",
    "    image = Image.open(image_path)\n",
    "  \n",
    "    scores, predictions = demo.predict(image_text.value, question_text.value)\n",
    "    scores = [score * 100 for score in scores]\n",
    "    df = pd.DataFrame({\n",
    "      \"Prediction\": predictions,\n",
    "      \"Confidence\": scores\n",
    "    })\n",
    "  \n",
    "    init_widgets(image_text.value, question_text.value)\n",
    "    display(image)\n",
    "\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e7d7fddb874b8c9caafdcfd2c51df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='http://images.cocodataset.org/train2017/000000505539.jpg', description='Image URL')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1da776a4b642bfaa5b09a9f882e8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='where is this place?', description='Question')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6bcc8f55f54d48b277f4765ece02be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Ask TextVQA!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_text, question_text = init_widgets(\n",
    "    \"http://images.cocodataset.org/train2017/000000505539.jpg\", \n",
    "    \"where is this place?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textvqa",
   "language": "python",
   "name": "textvqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
